{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext,load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置llm模型\n",
    "llm_name=\"qwen2.5:latest\"\n",
    "embedding_name=\"quentinz/bge-large-zh-v1.5:latest\"\n",
    "base_url='http://localhost:11434'\n",
    "\n",
    "Settings.llm = Ollama(\n",
    "    model=llm_name, \n",
    "    request_timeout=360.0,\n",
    "    base_url=base_url,\n",
    "    num_return_sequences=3)\n",
    "\n",
    "# 设置embedding model \n",
    "Settings.embed_model = OllamaEmbedding(\n",
    "    model_name=embedding_name,\n",
    "    base_url=base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents->nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextNode(id_='69d51383-7c92-4c5c-8506-9fddde60b1a3', embedding=None, metadata={'title': '', 'content_level': 2, 'images_path': ['images\\\\img110-110.png'], 'file_path': './preprocess\\\\doc112-安全出行-79.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='\\n## 打开/关闭全景天窗\\n\\n\\n手动滑动打开（轻按按钮至第1个停止位置）。\\n\\n自动滑动打开（按到底）。\\n\\n手动滑动关闭（轻按按钮至第1个停止位置）。\\n\\n自动滑动关闭（按到底）。\\n\\n如果全景天窗和遮阳帘处于完全关闭状态，轻按控制按钮，先打开遮\\n\\n阳帘，只有再次按下控制按钮后，才能打开全景天窗。\\n\\n如果全景天窗和遮阳帘处于完全关闭状态，短时间内将控制按钮按到\\n\\n底两次，遮阳帘和全景天窗同时打开。\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "def split_markdown_by_headers(markdown_text):\n",
    "    # 正则表达式匹配Markdown的一级和二级标题\n",
    "    headers = re.compile(r'^(#+) (.*)$', re.MULTILINE)\n",
    "    \n",
    "    # 用于存储结果的列表\n",
    "    headers_content = {}\n",
    "    current_block = []\n",
    "    current_title=''\n",
    "    no_content_h1=''\n",
    "    \n",
    "    # 按行分割文档\n",
    "    lines = markdown_text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        # 检查当前行是否是标题\n",
    "        match = headers.match(line)\n",
    "        if match:\n",
    "            # 如果是标题，保存当前块（如果有的话）\n",
    "            if current_block:\n",
    "                if len(current_block)<=2:\n",
    "                    no_content_h1=current_block[0]\n",
    "                else:\n",
    "                    headers_content[current_title]=no_content_h1+'\\n'+''.join(current_block)\n",
    "                    current_block=[]\n",
    "            current_title=match.string\n",
    "            \n",
    "        # 如果不是标题，添加到当前块\n",
    "        current_block.append(line + '\\n')\n",
    "    \n",
    "    # 添加最后一个块\n",
    "    headers_content[current_title]=''.join(current_block)\n",
    "    \n",
    "    return headers_content\n",
    "\n",
    "def get_block_images(block):\n",
    "    images_path = re.findall(r'!\\[.*?\\]\\((.*?)\\)', block)\n",
    "    return images_path\n",
    "\n",
    "def get_page_nodes(md_files,headers_content):\n",
    "    nodes=[]\n",
    "    \n",
    "    for header in headers_content:\n",
    "        # 获取block的图片\n",
    "        block=headers_content[header]\n",
    "        images_path=get_block_images(block)\n",
    "\n",
    "        # 去掉block的图片文本\n",
    "        pattern = r\"!\\[.*\\)\"\n",
    "        block= re.sub(pattern, \"\\n\", block)\n",
    "\n",
    "        # 添加metadata：标题、内容等级、图片路径\n",
    "        metadata={\n",
    "            'title': block.split('\\n')[0].replace('#',''),\n",
    "            'content_level': header.count('#') ,\n",
    "            'images_path': images_path,\n",
    "            'file_path': md_files\n",
    "        }\n",
    "        \n",
    " \n",
    "        node=TextNode(\n",
    "            text=block,\n",
    "            metadata=deepcopy(metadata),\n",
    "        )\n",
    "        nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "def get_nodes_by_documents():\n",
    "    documents=[]\n",
    "    md_files=glob.glob('./preprocess/*.md')\n",
    "    for md_file in md_files:\n",
    "        with open(md_file,encoding='utf-8') as fr:\n",
    "            md_content='\\n'.join(fr.readlines())\n",
    "        documents.append(md_content)\n",
    "    \n",
    "    nodes=[]\n",
    "    for md_file,document in zip(md_files,documents):\n",
    "        headers_content=split_markdown_by_headers(document)\n",
    "        document_nodes=get_page_nodes(md_file,headers_content)\n",
    "        nodes.extend(document_nodes)\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "\n",
    "nodes=get_nodes_by_documents()\n",
    "print(len(nodes))\n",
    "nodes[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nodes->indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(IndexType,nodes,persist_dir):\n",
    "    if os.path.exists(persist_dir):\n",
    "        storage_context=StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "        index=load_index_from_storage(storage_context=storage_context)\n",
    "    else:\n",
    "        index=IndexType(nodes=nodes,show_progress=True)\n",
    "        index.storage_context.persist(persist_dir=persist_dir)\n",
    "    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexs-> retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wushaogui\\miniconda3\\envs\\langchian\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from llama_index.core.storage.docstore.types import BaseDocumentStore\n",
    "from typing import Any, Callable, Dict, List, Optional, cast\n",
    "from llama_index.core.callbacks.base import CallbackManager\n",
    "from llama_index.core.constants import DEFAULT_SIMILARITY_TOP_K\n",
    "from llama_index.core.schema import BaseNode, IndexNode, NodeWithScore, QueryBundle\n",
    "from llama_index.core.vector_stores.utils import (\n",
    "    node_to_metadata_dict,\n",
    "    metadata_dict_to_node,\n",
    ")\n",
    "\n",
    "import itertools\n",
    "import jieba\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "class ChineseBM25Retriever(BM25Retriever):\n",
    "    \"\"\"A BM25 retriever that uses the BM25 algorithm to retrieve nodes.\n",
    "\n",
    "    Args:\n",
    "        nodes (List[BaseNode], optional):\n",
    "            The nodes to index. If not provided, an existing BM25 object must be passed.\n",
    "        similarity_top_k (int, optional):\n",
    "            The number of results to return. Defaults to DEFAULT_SIMILARITY_TOP_K.\n",
    "        callback_manager (CallbackManager, optional):\n",
    "            The callback manager to use. Defaults to None.\n",
    "        objects (List[IndexNode], optional):\n",
    "            The objects to retrieve. Defaults to None.\n",
    "        object_map (dict, optional):\n",
    "            A map of object IDs to nodes. Defaults to None.\n",
    "        verbose (bool, optional):\n",
    "            Whether to show progress. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def _chinese_tokenizer(self, texts: List[str]) -> tuple[str]:\n",
    "        # Use jieba to segment Chinese text\n",
    "        rslts = tuple(itertools.chain.from_iterable(jieba.cut(text) for text in texts))\n",
    "        return rslts\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            nodes: Optional[List[BaseNode]] = None,\n",
    "            similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n",
    "            callback_manager: Optional[CallbackManager] = None,\n",
    "            objects: Optional[List[IndexNode]] = None,\n",
    "            object_map: Optional[dict] = None,\n",
    "            verbose: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(\n",
    "            nodes=nodes,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            callback_manager=callback_manager,\n",
    "            objects=objects,\n",
    "            object_map=object_map,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        \n",
    "        # change the stop words for Chinese\n",
    "        with open(r'./stopwords-zh.txt', encoding='utf-8') as f: # here needs to add in the path of chinese stopwords\n",
    "            con = f.readlines()\n",
    "            stop_words = set()\n",
    "            for i in con:\n",
    "                i = i.rstrip('\\n')\n",
    "                stop_words.add(i)\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "        corpus_tokens = [\n",
    "            [word for word in jieba.cut_for_search(node.get_content()) if word not in stop_words and word.strip('\\n')]\n",
    "            for node in nodes\n",
    "        ]\n",
    "        corpus = [node_to_metadata_dict(node) for node in nodes]\n",
    "        self.bm25.corpus = corpus\n",
    "        self.bm25.index(corpus_tokens, show_progress=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_defaults(\n",
    "        cls,\n",
    "        index: Optional[VectorStoreIndex] = None,\n",
    "        nodes: Optional[List[BaseNode]] = None,\n",
    "        docstore: Optional[BaseDocumentStore] = None,\n",
    "        similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n",
    "        verbose: bool = False,\n",
    "    ) -> \"ChineseBM25Retriever\":\n",
    "          # ensure only one of index, nodes, or docstore is passed\n",
    "        if sum(bool(val) for val in [index, nodes, docstore]) != 1:\n",
    "            raise ValueError(\"Please pass exactly one of index, nodes, or docstore.\")\n",
    "\n",
    "        if index is not None:\n",
    "            docstore = index.docstore\n",
    "\n",
    "        if docstore is not None:\n",
    "            nodes = cast(List[BaseNode], list(docstore.docs.values()))\n",
    "        \n",
    "        assert (\n",
    "            nodes is not None\n",
    "        ), \"Please pass exactly one of index, nodes, or docstore.\"\n",
    "      \n",
    "        return cls(\n",
    "            nodes=nodes,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "      \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        query = query_bundle.query_str\n",
    "\n",
    "        tokenized_query = [[word for word in jieba.cut_for_search(query) if word not in self.stop_words]]\n",
    "\n",
    "        indexes, scores = self.bm25.retrieve(\n",
    "            tokenized_query, k=self.similarity_top_k, show_progress=self._verbose\n",
    "        )\n",
    "\n",
    "        # batched, but only one query\n",
    "        indexes = indexes[0]\n",
    "        scores = scores[0]\n",
    "\n",
    "        nodes: List[NodeWithScore] = []\n",
    "        for idx, score in zip(indexes, scores):\n",
    "            # idx can be an int or a dict of the node\n",
    "            if isinstance(idx, dict):\n",
    "                node = metadata_dict_to_node(idx)\n",
    "            else:\n",
    "                node_dict = self.corpus[int(idx)]\n",
    "                node = metadata_dict_to_node(node_dict)\n",
    "            nodes.append(NodeWithScore(node=node, score=float(score)))\n",
    "\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 563/563 [00:18<00:00, 30.07it/s]\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\WUSHAO~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.414 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.retrievers.fusion_retriever import FUSION_MODES\n",
    "\n",
    "\n",
    "index_type=VectorStoreIndex\n",
    "index_name=index_type.__name__\n",
    "index=build_index(index_type,nodes,os.path.join('./Storage',index_name))\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [\n",
    "        index.as_retriever(similarity_top_k=5),\n",
    "        ChineseBM25Retriever.from_defaults(\n",
    "            index=index,\n",
    "            similarity_top_k=3\n",
    "),\n",
    "    ],\n",
    "    num_queries=1,\n",
    "    use_async=True,\n",
    "    similarity_top_k=3,\n",
    "    mode=FUSION_MODES.RECIPROCAL_RANK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "response_synthesizer=get_response_synthesizer()\n",
    "\n",
    "query_engine=RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [02:00<00:00,  8.05s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import json\n",
    "\n",
    "with open('./data/test_question.json',encoding='utf-8') as fp:\n",
    "    test_questions=json.load(fp)\n",
    "\n",
    "answer_num=3\n",
    "for i,question_answers in enumerate(tqdm.tqdm(test_questions[:15])):\n",
    "    for j in range(answer_num):\n",
    "        question=question_answers['question']\n",
    "        response=query_engine.query(question)\n",
    "        question_answers[f'answer_{str(j+1)}']=response.response\n",
    "    test_questions[i]=question_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/test_question_answers.json',encoding='utf-8',mode='w') as fp:\n",
    "    json_str =json.dumps(test_questions,ensure_ascii=False,indent=4)\n",
    "    fp.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用CitationQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def generate_markdown_table(image_paths):\n",
    "    num_images = len(image_paths)\n",
    "    if num_images==0:\n",
    "        return ''\n",
    "    \n",
    "    num_cols = max(1,math.ceil(math.sqrt(num_images)))\n",
    "    num_rows = math.ceil(num_images / num_cols)\n",
    "\n",
    "    table = \"| \" + \" | \".join([\"-\"] * num_cols) + \" |\\n\"\n",
    "    table += \"| \" + \" | \".join([\"---\"] * num_cols) + \" |\\n\"\n",
    "\n",
    "    for row in range(num_rows):\n",
    "        row_images = image_paths[row * num_cols:(row + 1) * num_cols]\n",
    "        table += \"| \" + \" | \".join([f\"![Image](./preprocess/{img})\" for img in row_images])\n",
    "        table += \" |\" + \" |\" * (num_cols - len(row_images)) + \"\\n\"\n",
    "\n",
    "    return table\n",
    "\n",
    "def show_response(response):\n",
    "    md_content=f'问题：\\n{question}\\n'\n",
    "    md_content+='\\n---\\n'\n",
    "    md_content+='回答：\\n'\n",
    "\n",
    "    ref_string=''\n",
    "    ref_files=[]\n",
    "    imgs=[]\n",
    "    for ref in response.response.references:\n",
    "        try:\n",
    "            ref=int(ref)  # 或者 float(s)\n",
    "        except ValueError:\n",
    "            print(\"不是数字\")\n",
    "            continue\n",
    "\n",
    "        # 图片内容\n",
    "        imgs.extend(response.source_nodes[ref-1].metadata['images_path'])\n",
    "\n",
    "        # 引用内容\n",
    "        file_path=response.source_nodes[ref-1].metadata['file_path']\n",
    "        if file_path not in ref_files:\n",
    "            ref_files.append(file_path)\n",
    "            ref_string+=f'{ref}. [{os.path.splitext(os.path.basename(file_path))[0]}]({file_path})\\n'\n",
    "        \n",
    "    \n",
    "    # 图片\n",
    "    md_content+='\\n'\n",
    "    md_content+=generate_markdown_table(imgs)\n",
    "\n",
    "    # 回答\n",
    "    md_content+='\\n\\n'\n",
    "    md_content+=response.response.response\n",
    "    \n",
    "    # 引用\n",
    "    md_content+='\\n\\n参考：\\n\\n'\n",
    "    md_content+=ref_string\n",
    "    \n",
    "    # print(md_content)\n",
    "    display(Markdown(md_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    \"\"\"Output containing the response, references.\"\"\"\n",
    "\n",
    "    response: str = Field(..., description=\"The answer to the question.\")\n",
    "    references: List[int] = Field(\n",
    "        ...,\n",
    "        description=\"Which context is referenced to generate the content of the answer\",\n",
    "    )\n",
    "    \n",
    "llm = Ollama(\n",
    "    model=llm_name, \n",
    "    request_timeout=360.0,\n",
    "    base_url=base_url)\n",
    "\n",
    "sllm = llm.as_structured_llm(output_cls=Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine= index.as_query_engine(similarity_top_k=3)\n",
    "from llama_index.core.query_engine import CitationQueryEngine\n",
    "citation_query_engine = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    llm=sllm,\n",
    "    retriever=retriever,\n",
    "    similarity_top_k=5,\n",
    "    citation_chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "问题：\n",
       "如何通过空调系统面板调节空调风量？\n",
       "\n",
       "---\n",
       "回答：\n",
       "\n",
       "| - |\n",
       "| --- |\n",
       "| ![Image](./preprocess/images\\img244-273.png) |\n",
       "\n",
       "\n",
       "您可以通过按压空调系统面板上的‘01 风速/副驾温度调节切换按钮’并旋转对应的‘02 风速/副驾温度调节旋钮’来调节空调的风量。当图标亮起时，可以开启/关闭鼓风机以及调整鼓风机转速，共有9个速度等级可设置。\n",
       "\n",
       "参考：\n",
       "1. [doc244-空调-157](./preprocess\\doc244-空调-157.md)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question='如何通过空调系统面板调节空调风量？'\n",
    "response=citation_query_engine.query(question)\n",
    "show_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "问题：\n",
       "车辆尾门的防夹保护功能是如何工作的？\n",
       "\n",
       "---\n",
       "回答：\n",
       "\n",
       "\n",
       "\n",
       "车辆尾门的防夹保护功能在尾门关闭过程中遇到障碍物时会停止动作并保持当前位置，同时伴有长鸣音。具体来说，在车辆运动状态下如果尾门触碰障碍物，则仅会停止关闭；而在车辆静止状态下则会被迫打开至设定的开启位置 [2]。此外，当尾门在打开过程中碰到障碍物时，它也会停止并在当前位置保持不动，并伴随有长鸣音警告[1]。\n",
       "\n",
       "参考：\n",
       "2. [doc34-装载货物-25](./preprocess\\doc34-装载货物-25.md)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question='车辆尾门的防夹保护功能是如何工作的？'\n",
    "response=citation_query_engine.query(question)\n",
    "show_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "问题：\n",
       "如何创建人脸识别？\n",
       "\n",
       "---\n",
       "回答：\n",
       "\n",
       "| - | - |\n",
       "| --- | --- |\n",
       "| ![Image](./preprocess/images\\img270-316.png) | ![Image](./preprocess/images\\img270-317.png) |\n",
       "\n",
       "\n",
       "要创建人脸识别，请按照以下步骤操作：\n",
       "\n",
       "1. 点击开启/关闭人脸识别功能。\n",
       "2. 点击图标，添加人脸信息。\n",
       "\n",
       "注意，在创建人脸信息时，请将车辆挡位切换至驻车挡（P），并确保面部五官清晰可见，避免佩戴帽子、墨镜或口罩等物品。此外，如果创建成功或失败，中央显示屏会提示相关信息。\n",
       "\n",
       "参考：\n",
       "\n",
       "2. [doc271-中央显示屏-172](./preprocess\\doc271-中央显示屏-172.md)\n",
       "3. [doc273-中央显示屏-173](./preprocess\\doc273-中央显示屏-173.md)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question='如何创建人脸识别？'\n",
    "response=citation_query_engine.query(question)\n",
    "show_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "问题：\n",
       "如何调节外后视镜？\n",
       "\n",
       "---\n",
       "回答：\n",
       "\n",
       "| - | - |\n",
       "| --- | --- |\n",
       "| ![Image](./preprocess/images\\img60-55.png) | ![Image](./preprocess/images\\img60-56.png) |\n",
       "\n",
       "\n",
       "要调节外后视镜，您可以按照以下步骤操作：1. 按下L按钮来调整左侧外后视镜；按下R按钮来调整右侧外后视镜。在相应的按钮上会点亮指示灯。2. 使用中间的控制杆来进一步调整外后视镜的角度。3. 再次按下L或R按钮，此时指示灯将熄灭以完成调节。注意，在驾驶过程中请勿调节外后视镜[1]。\n",
       "\n",
       "参考：\n",
       "\n",
       "1. [doc60-驾驶前的准备-45](./preprocess\\doc60-驾驶前的准备-45.md)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question='如何调节外后视镜？'\n",
    "response=citation_query_engine.query(question)\n",
    "show_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
