{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext,load_index_from_storage\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_qa_embedding_pairs,\n",
    "    EmbeddingQAFinetuneDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置llm模型\n",
    "llm_name=\"qwen2.5:latest\"\n",
    "embedding_name=\"quentinz/bge-large-zh-v1.5:latest\"\n",
    "base_url='http://localhost:11434'\n",
    "\n",
    "Settings.llm = Ollama(\n",
    "    model=llm_name, \n",
    "    request_timeout=360.0,\n",
    "    base_url=base_url)\n",
    "\n",
    "# 设置embedding model \n",
    "Settings.embed_model = OllamaEmbedding(\n",
    "    model_name=embedding_name,\n",
    "    base_url=base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents->nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563 Node ID: edf882a8-9b37-4524-bcbd-a1a39b04b6e2\n",
      "Text: ## 打开/关闭全景天窗   手动滑动打开（轻按按钮至第1个停止位置）。  自动滑动打开（按到底）。\n",
      "手动滑动关闭（轻按按钮至第1个停止位置）。  自动滑动关闭（按到底）。  如果全景天窗和遮阳帘处于完全关闭状态，轻按控制按钮，先打开遮\n",
      "阳帘，只有再次按下控制按钮后，才能打开全景天窗。  如果全景天窗和遮阳帘处于完全关闭状态，短时间内将控制按钮按到\n",
      "底两次，遮阳帘和全景天窗同时打开。\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from copy import deepcopy\n",
    "from llama_index.core.schema import Document,TextNode\n",
    "\n",
    "def split_markdown_by_headers(markdown_text):\n",
    "    # 正则表达式匹配Markdown的一级和二级标题\n",
    "    headers = re.compile(r'^(#+) (.*)$', re.MULTILINE)\n",
    "    \n",
    "    # 用于存储结果的列表\n",
    "    headers_content = {}\n",
    "    current_block = []\n",
    "    current_title=''\n",
    "    no_content_h1=''\n",
    "    \n",
    "    # 按行分割文档\n",
    "    lines = markdown_text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        # 检查当前行是否是标题\n",
    "        match = headers.match(line)\n",
    "        if match:\n",
    "            # 如果是标题，保存当前块（如果有的话）\n",
    "            if current_block:\n",
    "                if len(current_block)<=2:\n",
    "                    no_content_h1=current_block[0]\n",
    "                else:\n",
    "                    headers_content[current_title]=no_content_h1+'\\n'+''.join(current_block)\n",
    "                    current_block=[]\n",
    "            current_title=match.string\n",
    "            \n",
    "        # 如果不是标题，添加到当前块\n",
    "        current_block.append(line + '\\n')\n",
    "    \n",
    "    # 添加最后一个块\n",
    "    headers_content[current_title]=''.join(current_block)\n",
    "    \n",
    "    return headers_content\n",
    "\n",
    "def get_block_images(block):\n",
    "    images_path = re.findall(r'!\\[.*?\\]\\((.*?)\\)', block)\n",
    "    return images_path\n",
    "\n",
    "def get_page_nodes(headers_content,separator=\"\"):\n",
    "    nodes=[]\n",
    "    \n",
    "    for header in headers_content:\n",
    "        # 获取block的图片\n",
    "        block=headers_content[header]\n",
    "        images_path=get_block_images(block)\n",
    "\n",
    "        # 去掉block的图片文本\n",
    "        pattern = r\"!\\[.*\\)\"\n",
    "        block= re.sub(pattern, \"\\n\", block)\n",
    "\n",
    "        # 添加metadata：标题、内容等级、图片路径\n",
    "        metadata={\n",
    "            'title': block.split('\\n')[0].replace('#',''),\n",
    "            'content_level': header.count('#') ,\n",
    "            'images_path': images_path\n",
    "        }\n",
    "        \n",
    " \n",
    "        node=TextNode(\n",
    "            text=block,\n",
    "            metadata=deepcopy(metadata),\n",
    "        )\n",
    "        nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "def get_nodes_by_documents():\n",
    "    documents=[]\n",
    "    md_files=glob.glob('./preprocess/*.md')\n",
    "    for md_file in md_files:\n",
    "        with open(md_file,encoding='utf-8') as fr:\n",
    "            md_content='\\n'.join(fr.readlines())\n",
    "        documents.append(md_content)\n",
    "    \n",
    "    nodes=[]\n",
    "    for document in documents:\n",
    "        headers_content=split_markdown_by_headers(document)\n",
    "        document_nodes=get_page_nodes(headers_content)\n",
    "        nodes.extend(document_nodes)\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "\n",
    "nodes=get_nodes_by_documents()\n",
    "print(len(nodes),nodes[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext,load_index_from_storage\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_qa_embedding_pairs,\n",
    "    EmbeddingQAFinetuneDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置llm模型\n",
    "llm_name=\"qwen2.5:latest\"\n",
    "embedding_name=\"quentinz/bge-large-zh-v1.5:latest\"\n",
    "base_url='http://localhost:11434'\n",
    "\n",
    "Settings.llm = Ollama(\n",
    "    model=llm_name, \n",
    "    request_timeout=360.0,\n",
    "    base_url=base_url)\n",
    "\n",
    "# 设置embedding model \n",
    "Settings.embed_model = OllamaEmbedding(\n",
    "    model_name=embedding_name,\n",
    "    base_url=base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nodes->indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DocumentSummaryIndex', 'EmptyIndex', 'GPTDocumentSummaryIndex', 'GPTEmptyIndex', 'GPTKeywordTableIndex', 'GPTListIndex', 'GPTPandasIndex', 'GPTRAKEKeywordTableIndex', 'GPTSQLStructStoreIndex', 'GPTSimpleKeywordTableIndex', 'GPTTreeIndex', 'GPTVectorStoreIndex', 'KeywordTableIndex', 'KnowledgeGraphIndex', 'ListIndex', 'MultiModalVectorStoreIndex', 'PandasIndex', 'PropertyGraphIndex', 'RAKEKeywordTableIndex', 'SQLStructStoreIndex', 'SimpleKeywordTableIndex', 'SummaryIndex', 'TreeIndex', 'VectorStoreIndex']\n"
     ]
    }
   ],
   "source": [
    "# 查看所有的Index\n",
    "from llama_index.core import indices\n",
    "\n",
    "indexs=list(filter(lambda att:att.find('Index')>0,dir(indices)))\n",
    "print(indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(IndexType,nodes,persist_dir):\n",
    "    if os.path.exists(persist_dir):\n",
    "        storage_context=StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "        index=load_index_from_storage(storage_context=storage_context)\n",
    "    else:\n",
    "        index=IndexType(nodes=nodes,show_progress=True)\n",
    "        index.storage_context.persist(persist_dir=persist_dir)\n",
    "    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexs-> retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wushaogui\\miniconda3\\envs\\langchian\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from llama_index.core.storage.docstore.types import BaseDocumentStore\n",
    "from typing import Any, Callable, Dict, List, Optional, cast\n",
    "from llama_index.core.callbacks.base import CallbackManager\n",
    "from llama_index.core.constants import DEFAULT_SIMILARITY_TOP_K\n",
    "from llama_index.core.schema import BaseNode, IndexNode, NodeWithScore, QueryBundle\n",
    "from llama_index.core.vector_stores.utils import (\n",
    "    node_to_metadata_dict,\n",
    "    metadata_dict_to_node,\n",
    ")\n",
    "\n",
    "import itertools\n",
    "import jieba\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "class ChineseBM25Retriever(BM25Retriever):\n",
    "    \"\"\"A BM25 retriever that uses the BM25 algorithm to retrieve nodes.\n",
    "\n",
    "    Args:\n",
    "        nodes (List[BaseNode], optional):\n",
    "            The nodes to index. If not provided, an existing BM25 object must be passed.\n",
    "        similarity_top_k (int, optional):\n",
    "            The number of results to return. Defaults to DEFAULT_SIMILARITY_TOP_K.\n",
    "        callback_manager (CallbackManager, optional):\n",
    "            The callback manager to use. Defaults to None.\n",
    "        objects (List[IndexNode], optional):\n",
    "            The objects to retrieve. Defaults to None.\n",
    "        object_map (dict, optional):\n",
    "            A map of object IDs to nodes. Defaults to None.\n",
    "        verbose (bool, optional):\n",
    "            Whether to show progress. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def _chinese_tokenizer(self, texts: List[str]) -> tuple[str]:\n",
    "        # Use jieba to segment Chinese text\n",
    "        rslts = tuple(itertools.chain.from_iterable(jieba.cut(text) for text in texts))\n",
    "        return rslts\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            nodes: Optional[List[BaseNode]] = None,\n",
    "            similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n",
    "            callback_manager: Optional[CallbackManager] = None,\n",
    "            objects: Optional[List[IndexNode]] = None,\n",
    "            object_map: Optional[dict] = None,\n",
    "            verbose: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(\n",
    "            nodes=nodes,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            callback_manager=callback_manager,\n",
    "            objects=objects,\n",
    "            object_map=object_map,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        \n",
    "        # change the stop words for Chinese\n",
    "        with open(r'./stopwords-zh.txt', encoding='utf-8') as f: # here needs to add in the path of chinese stopwords\n",
    "            con = f.readlines()\n",
    "            stop_words = set()\n",
    "            for i in con:\n",
    "                i = i.rstrip('\\n')\n",
    "                stop_words.add(i)\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "        corpus_tokens = [\n",
    "            [word for word in jieba.cut_for_search(node.get_content()) if word not in stop_words and word.strip('\\n')]\n",
    "            for node in nodes\n",
    "        ]\n",
    "        corpus = [node_to_metadata_dict(node) for node in nodes]\n",
    "        self.bm25.corpus = corpus\n",
    "        self.bm25.index(corpus_tokens, show_progress=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_defaults(\n",
    "        cls,\n",
    "        index: Optional[VectorStoreIndex] = None,\n",
    "        nodes: Optional[List[BaseNode]] = None,\n",
    "        docstore: Optional[BaseDocumentStore] = None,\n",
    "        similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n",
    "        verbose: bool = False,\n",
    "    ) -> \"ChineseBM25Retriever\":\n",
    "          # ensure only one of index, nodes, or docstore is passed\n",
    "        if sum(bool(val) for val in [index, nodes, docstore]) != 1:\n",
    "            raise ValueError(\"Please pass exactly one of index, nodes, or docstore.\")\n",
    "\n",
    "        if index is not None:\n",
    "            docstore = index.docstore\n",
    "\n",
    "        if docstore is not None:\n",
    "            nodes = cast(List[BaseNode], list(docstore.docs.values()))\n",
    "        \n",
    "        assert (\n",
    "            nodes is not None\n",
    "        ), \"Please pass exactly one of index, nodes, or docstore.\"\n",
    "      \n",
    "        return cls(\n",
    "            nodes=nodes,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "      \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        query = query_bundle.query_str\n",
    "\n",
    "        tokenized_query = [[word for word in jieba.cut_for_search(query) if word not in self.stop_words]]\n",
    "\n",
    "        indexes, scores = self.bm25.retrieve(\n",
    "            tokenized_query, k=self.similarity_top_k, show_progress=self._verbose\n",
    "        )\n",
    "\n",
    "        # batched, but only one query\n",
    "        indexes = indexes[0]\n",
    "        scores = scores[0]\n",
    "\n",
    "        nodes: List[NodeWithScore] = []\n",
    "        for idx, score in zip(indexes, scores):\n",
    "            # idx can be an int or a dict of the node\n",
    "            if isinstance(idx, dict):\n",
    "                node = metadata_dict_to_node(idx)\n",
    "            else:\n",
    "                node_dict = self.corpus[int(idx)]\n",
    "                node = metadata_dict_to_node(node_dict)\n",
    "            nodes.append(NodeWithScore(node=node, score=float(score)))\n",
    "\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\WUSHAO~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.449 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.retrievers.fusion_retriever import FUSION_MODES\n",
    "\n",
    "\n",
    "index_type=VectorStoreIndex\n",
    "index_name=index_type.__name__\n",
    "index=build_index(index_type,nodes,os.path.join('./Storage',index_name))\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [\n",
    "        index.as_retriever(similarity_top_k=5),\n",
    "        ChineseBM25Retriever.from_defaults(\n",
    "            index=index,\n",
    "            similarity_top_k=3\n",
    "),\n",
    "    ],\n",
    "    num_queries=1,\n",
    "    use_async=True,\n",
    "    similarity_top_k=3,\n",
    "    mode=FUSION_MODES.RECIPROCAL_RANK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "response_synthesizer=get_response_synthesizer()\n",
    "\n",
    "query_engine=RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"500px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "from llama_index.core.prompts.prompt_type import PromptType\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.llama_dataset import RagPredictionDataset\n",
    "\n",
    "async def Build_test_dataset(test_size=10,data_dir='./TestData/'):\n",
    "    ragdataset_path=os.path.join(data_dir,'ragdataset.json')\n",
    "    ragdataset_predictions_path=os.path.join(data_dir,'ragdataset_predictions.jaon')\n",
    "\n",
    "    if os.path.exists(ragdataset_path) and os.path.exists(ragdataset_predictions_path):\n",
    "        rag_dataset=LabeledRagDataset.from_json(ragdataset_path)\n",
    "        rag_predictions_dataset=RagPredictionDataset.from_json(ragdataset_predictions_path)\n",
    "    else:\n",
    "        DEFAULT_QUESTION_GENERATION_PROMPT = \"\"\"\\\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "        {context_str}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge.\n",
    "        generate only questions based on the below query.\n",
    "        使用中文生成答案\n",
    "        {query_str}\n",
    "        \"\"\"\n",
    "\n",
    "        DEFAULT_TEXT_QA_PROMPT_TMPL=(\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information and not prior knowledge,answer the query.\\n\"\n",
    "            \"使用中文生成答案\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "\n",
    "        text_qa_template = PromptTemplate(\n",
    "            DEFAULT_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER\n",
    "        )\n",
    "\n",
    "        text_question_template=PromptTemplate(DEFAULT_QUESTION_GENERATION_PROMPT)\n",
    "\n",
    "        num_questions_per_chunk=1\n",
    "        # role=\"Teacher/Professor\"\n",
    "        role=\"资深汽车售后工程师\"\n",
    "        question_gen_query=f\"\"\"\n",
    "            You are a {role}. \n",
    "            Your task is to setup {num_questions_per_chunk} questions for an upcoming quiz/examination. \n",
    "            The questions should be diverse in nature across the document. \n",
    "            Restrict the questions to the context information provided. \n",
    "        \"\"\"\n",
    "\n",
    "        import random\n",
    "        random.seed(0)\n",
    "        test_size=min(len(nodes),test_size)\n",
    "        sample_nodes=random.sample(nodes,test_size)\n",
    "\n",
    "        # step1:初始化数据生成器\n",
    "        print('step1:初始化数据生成器')\n",
    "        rag_dataset_generator=RagDatasetGenerator(nodes=sample_nodes,\n",
    "                                                text_question_template=text_question_template,\n",
    "                                                text_qa_template=text_qa_template,\n",
    "                                                question_gen_query=question_gen_query,\n",
    "                                                num_questions_per_chunk=num_questions_per_chunk,\n",
    "                                                show_progress=True)\n",
    "\n",
    "        # step2:为每个node生成问题（包含标准答案）\n",
    "        print('step2:为每个node生成问题（包含标准答案）')\n",
    "        rag_dataset = rag_dataset_generator.generate_dataset_from_nodes()\n",
    "        rag_dataset.save_json(ragdataset_path)\n",
    "\n",
    "        # step3:使用query_engine回答问题\n",
    "        print('step3:使用query_engine回答问题')\n",
    "        rag_predictions_dataset=await rag_dataset.amake_predictions_with(\n",
    "            predictor=query_engine,\n",
    "            batch_size=2,\n",
    "            show_progress=True,\n",
    "            sleep_time_in_seconds=2)\n",
    "        rag_predictions_dataset.save_json(ragdataset_predictions_path)\n",
    "    \n",
    "    return rag_dataset,rag_predictions_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset,rag_predictions_dataset=await Build_test_dataset(test_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_eb6cc_row0_col0, #T_eb6cc_row0_col1, #T_eb6cc_row0_col2, #T_eb6cc_row1_col0, #T_eb6cc_row1_col1, #T_eb6cc_row1_col2 {\n",
       "  inline-size: 500px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_eb6cc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_eb6cc_level0_col0\" class=\"col_heading level0 col0\" >上下文</th>\n",
       "      <th id=\"T_eb6cc_level0_col1\" class=\"col_heading level0 col1\" >生成的提问</th>\n",
       "      <th id=\"T_eb6cc_level0_col2\" class=\"col_heading level0 col2\" >回答</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_eb6cc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_eb6cc_row0_col0\" class=\"data row0 col0\" >['# 检查制动液\\n\\n1 检查制动液液位。\\n\\n\\n请不定期地检查制动液液位，确保液位在MIN（最低）和MAX（最\\n\\n高）标记之间。\\n\\n请参见电子版用户手册保养项目规定的间隔时间定期更换制动液。\\n\\nLynk & Co领克建议您请前往Lynk & Co领克中心更换制动液。\\n\\n警告！\\n\\n■如果制动液液位低于储液罐的最低液位，请勿驾驶车辆。如果制\\n\\n动液液位下降明显，制动系统可能出现故障，联系Lynk & Co领克\\n\\n中心进行检查。\\n\\n■制动液有毒。请保持制动液容器密封，避免儿童接触。如误服制\\n\\n动液，请立即就医。\\n\\n■如制动液与皮肤接触或进入眼睛，请立即用大量清水冲洗。\\n\\n注意！\\n\\n■建议使用Lynk & Co领克原厂纯正“DOT4”制动液。\\n\\n■请务必使用处于保质期内且未使用过的制动液。\\n\\n']</td>\n",
       "      <td id=\"T_eb6cc_row0_col1\" class=\"data row0 col1\" >根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。</td>\n",
       "      <td id=\"T_eb6cc_row0_col2\" class=\"data row0 col2\" >如果制动液液位低于储液罐的最低液位，应立即停止驾驶，并联系Lynk & Co领克中心进行检查。不要继续使用车辆，因为制动液液位下降明显可能意味着制动系统存在故障。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb6cc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_eb6cc_row1_col0\" class=\"data row1 col0\" >['\\n## 手动SOS呼叫\\n\\n按住\\n\\n按键约2 秒，手动激活SOS功能。\\n\\nSOS功能激活后，乘员可直接向Lynk & Co领克客户联络中心寻求紧急\\n\\n援助。如果Lynk & Co领克客户联络中心未得到任何回应，车辆位置将\\n\\n发送到Lynk & Co领克客户联络中心。Lynk & Co领克客户联络中心将\\n\\n提供适当的援助（呼叫救护车、警察等）。\\n\\n']</td>\n",
       "      <td id=\"T_eb6cc_row1_col1\" class=\"data row1 col1\" >在激活SOS功能时，需要按住哪个按键多长时间才能手动激活该功能？当Lynk & Co领克客户联络中心未得到任何回应时，车辆会发送什么信息？（2分）</td>\n",
       "      <td id=\"T_eb6cc_row1_col2\" class=\"data row1 col2\" >在激活SOS功能时，需要按住特定按键约2秒才能手动激活该功能。当Lynk & Co领克客户联络中心未得到任何回应时，车辆位置信息将被发送到Lynk & Co领克客户联络中心。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x215dbbd46d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 查看测试数据\n",
    "show_num=2\n",
    "contexts_query_answer={\n",
    "    \"上下文\":[example.reference_contexts for example in rag_dataset.examples[:show_num]],\n",
    "    \"生成的提问\":[example.query for example in rag_dataset.examples[:show_num]],\n",
    "    \"回答\":[example.response for example in rag_predictions_dataset.predictions[:show_num]],\n",
    "}\n",
    "\n",
    "df=pd.DataFrame(contexts_query_answer)\n",
    "displayify_df(df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AnswerRelevancyEvaluator', 'BaseEvaluator', 'BaseRetrievalEvaluator', 'ContextRelevancyEvaluator', 'CorrectnessEvaluator', 'FaithfulnessEvaluator', 'GuidelineEvaluator', 'MultiModalRetrieverEvaluator', 'PairwiseComparisonEvaluator', 'QueryResponseEvaluator', 'RelevancyEvaluator', 'ResponseEvaluator', 'RetrieverEvaluator', 'SemanticSimilarityEvaluator']\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import evaluation\n",
    "\n",
    "evaluations=list(filter(lambda att:att.find('Evaluator')>0,dir(evaluation)))\n",
    "print(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate the gpt-4 judges\n",
    "from llama_index.core.evaluation import (\n",
    "    AnswerRelevancyEvaluator,\n",
    "    ContextRelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "judges[\"answer_relevancy\"] = AnswerRelevancyEvaluator()\n",
    "judges[\"context_relevancy\"] = ContextRelevancyEvaluator()\n",
    "judges[\"correctness\"] = CorrectnessEvaluator()\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 测试集\n",
    "examples_with_prediction=list(zip(rag_dataset.examples, rag_predictions_dataset.predictions))\n",
    "\n",
    "eval_tasks = []\n",
    "for example, prediction in examples_with_prediction:\n",
    "    eval_tasks.append(\n",
    "        judges[\"answer_relevancy\"].aevaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )\n",
    "    eval_tasks.append(\n",
    "        judges[\"context_relevancy\"].aevaluate(\n",
    "            query=example.query,\n",
    "            contexts=prediction.contexts,\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    eval_tasks.append(\n",
    "        judges[\"correctness\"].aevaluate(\n",
    "            query=example.query,\n",
    "            contexts=prediction.contexts,\n",
    "            response=prediction.response,\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    eval_tasks.append(\n",
    "        judges[\"faithfulness\"].aevaluate(\n",
    "            query=example.query,\n",
    "            contexts=prediction.contexts,\n",
    "            response=prediction.response,\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:07<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.asyncio import tqdm_asyncio\n",
    "eval_results = await tqdm_asyncio.gather(*eval_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分指标\n",
    "evals = {\n",
    "    \"answer_relevancy\": eval_results[::4],\n",
    "    \"context_relevancy\": eval_results[1::4],\n",
    "    \"correctness\": eval_results[2::4],\n",
    "    \"faithfulness\": eval_results[3::4],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "import pandas as pd\n",
    "\n",
    "deep_dfs = {}\n",
    "mean_dfs = {}\n",
    "for metric in evals.keys():\n",
    "    deep_df, mean_df = get_eval_results_df(\n",
    "        names=[\"baseline\"] * len(evals[metric]),\n",
    "        results_arr=evals[metric],\n",
    "        metric=metric,\n",
    "    )\n",
    "    deep_dfs[metric] = deep_df\n",
    "    mean_dfs[metric] = mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_answer_relevancy_score</th>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_relevancy_score</th>\n",
       "      <td>0.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                           baseline\n",
       "metrics                               \n",
       "mean_answer_relevancy_score   0.950000\n",
       "mean_context_relevancy_score  0.859375\n",
       "mean_correctness_score        3.600000\n",
       "mean_faithfulness_score       0.700000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores_df = pd.concat(\n",
    "    [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rag</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>scores</th>\n",
       "      <th>feedbacks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。</td>\n",
       "      <td>如果制动液液位低于储液罐的最低液位，应立即停止驾驶，并联系Lynk &amp; Co领克中心进行检查...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1. **Does the provided response match the subj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline</td>\n",
       "      <td>在激活SOS功能时，需要按住哪个按键多长时间才能手动激活该功能？当Lynk &amp; Co领克客户...</td>\n",
       "      <td>在激活SOS功能时，需要按住特定按键约2秒才能手动激活该功能。当Lynk &amp; Co领克客户联...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1. **Does the provided response match the subj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline</td>\n",
       "      <td>如何通过Lynk &amp; Co App操作前排座椅加热功能？请描述具体步骤。</td>\n",
       "      <td>要通过Lynk &amp; Co App操作前排座椅加热功能，可以按照以下步骤进行：\\n\\n1. 登...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1. 该响应匹配用户查询的主题。用户询问如何通过Lynk &amp; Co App操作前排座椅加热功...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rag                                              query  \\\n",
       "0  baseline        根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。   \n",
       "1  baseline  在激活SOS功能时，需要按住哪个按键多长时间才能手动激活该功能？当Lynk & Co领克客户...   \n",
       "2  baseline               如何通过Lynk & Co App操作前排座椅加热功能？请描述具体步骤。   \n",
       "\n",
       "                                              answer contexts  scores  \\\n",
       "0  如果制动液液位低于储液罐的最低液位，应立即停止驾驶，并联系Lynk & Co领克中心进行检查...     None     1.0   \n",
       "1  在激活SOS功能时，需要按住特定按键约2秒才能手动激活该功能。当Lynk & Co领克客户联...     None     1.0   \n",
       "2  要通过Lynk & Co App操作前排座椅加热功能，可以按照以下步骤进行：\\n\\n1. 登...     None     1.0   \n",
       "\n",
       "                                           feedbacks  \n",
       "0  1. **Does the provided response match the subj...  \n",
       "1  1. **Does the provided response match the subj...  \n",
       "2  1. 该响应匹配用户查询的主题。用户询问如何通过Lynk & Co App操作前排座椅加热功...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_dfs[\"answer_relevancy\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rag</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>scores</th>\n",
       "      <th>feedbacks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。</td>\n",
       "      <td>None</td>\n",
       "      <td>[# 检查制动液\\n\\n1 检查制动液液位。\\n\\n\\n请不定期地检查制动液液位，确保液位在...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1. **Does the retrieved context match the subj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline</td>\n",
       "      <td>在激活SOS功能时，需要按住哪个按键多长时间才能手动激活该功能？当Lynk &amp; Co领克客户...</td>\n",
       "      <td>None</td>\n",
       "      <td>[\\n## 手动SOS呼叫\\n\\n按住\\n\\n按键约2 秒，手动激活SOS功能。\\n\\nSO...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>### 评价步骤：\\n\\n#### 第一步：主题匹配度评估\\n- **查询内容**：涉及两个...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline</td>\n",
       "      <td>如何通过Lynk &amp; Co App操作前排座椅加热功能？请描述具体步骤。</td>\n",
       "      <td>None</td>\n",
       "      <td>[# 前排座椅加热\\n\\n## 使用Lynk &amp; Co App打开/关闭前排座椅加热\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1. **Does the retrieved context match the subj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rag                                              query answer  \\\n",
       "0  baseline        根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。   None   \n",
       "1  baseline  在激活SOS功能时，需要按住哪个按键多长时间才能手动激活该功能？当Lynk & Co领克客户...   None   \n",
       "2  baseline               如何通过Lynk & Co App操作前排座椅加热功能？请描述具体步骤。   None   \n",
       "\n",
       "                                            contexts  scores  \\\n",
       "0  [# 检查制动液\\n\\n1 检查制动液液位。\\n\\n\\n请不定期地检查制动液液位，确保液位在...     NaN   \n",
       "1  [\\n## 手动SOS呼叫\\n\\n按住\\n\\n按键约2 秒，手动激活SOS功能。\\n\\nSO...     1.0   \n",
       "2  [# 前排座椅加热\\n\\n## 使用Lynk & Co App打开/关闭前排座椅加热\\n\\n...     NaN   \n",
       "\n",
       "                                           feedbacks  \n",
       "0  1. **Does the retrieved context match the subj...  \n",
       "1  ### 评价步骤：\\n\\n#### 第一步：主题匹配度评估\\n- **查询内容**：涉及两个...  \n",
       "2  1. **Does the retrieved context match the subj...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_dfs[\"context_relevancy\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rag</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>scores</th>\n",
       "      <th>feedbacks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。</td>\n",
       "      <td>如果制动液液位低于储液罐的最低液位，应立即停止驾驶，并联系Lynk &amp; Co领克中心进行检查...</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>该回答直接且明确地指出了当制动液液位低于储液罐最低液位时应采取的措施，并强调了立即停车和联系...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline</td>\n",
       "      <td>在激活SOS功能时，需要按住哪个按键多长时间才能手动激活该功能？当Lynk &amp; Co领克客户...</td>\n",
       "      <td>在激活SOS功能时，需要按住特定按键约2秒才能手动激活该功能。当Lynk &amp; Co领克客户联...</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>生成的答案针对了两个问题中的一个，并且提供了准确的信息。对于激活SOS功能所需按键的按压时间...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline</td>\n",
       "      <td>如何通过Lynk &amp; Co App操作前排座椅加热功能？请描述具体步骤。</td>\n",
       "      <td>要通过Lynk &amp; Co App操作前排座椅加热功能，可以按照以下步骤进行：\\n\\n1. 登...</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>该回答基本符合问题的需求，并且提供了一般性的操作步骤。但是，步骤过于简略，缺乏具体细节，可能...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rag                                              query  \\\n",
       "0  baseline        根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。   \n",
       "1  baseline  在激活SOS功能时，需要按住哪个按键多长时间才能手动激活该功能？当Lynk & Co领克客户...   \n",
       "2  baseline               如何通过Lynk & Co App操作前排座椅加热功能？请描述具体步骤。   \n",
       "\n",
       "                                              answer contexts  scores  \\\n",
       "0  如果制动液液位低于储液罐的最低液位，应立即停止驾驶，并联系Lynk & Co领克中心进行检查...     None     4.0   \n",
       "1  在激活SOS功能时，需要按住特定按键约2秒才能手动激活该功能。当Lynk & Co领克客户联...     None     4.0   \n",
       "2  要通过Lynk & Co App操作前排座椅加热功能，可以按照以下步骤进行：\\n\\n1. 登...     None     3.0   \n",
       "\n",
       "                                           feedbacks  \n",
       "0  该回答直接且明确地指出了当制动液液位低于储液罐最低液位时应采取的措施，并强调了立即停车和联系...  \n",
       "1  生成的答案针对了两个问题中的一个，并且提供了准确的信息。对于激活SOS功能所需按键的按压时间...  \n",
       "2  该回答基本符合问题的需求，并且提供了一般性的操作步骤。但是，步骤过于简略，缺乏具体细节，可能...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_dfs[\"correctness\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rag</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>scores</th>\n",
       "      <th>feedbacks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。</td>\n",
       "      <td>如果制动液液位低于储液罐的最低液位，应立即停止驾驶，并联系Lynk &amp; Co领克中心进行检查...</td>\n",
       "      <td>[# 检查制动液\\n\\n1 检查制动液液位。\\n\\n\\n请不定期地检查制动液液位，确保液位在...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline</td>\n",
       "      <td>在激活SOS功能时，需要按住哪个按键多长时间才能手动激活该功能？当Lynk &amp; Co领克客户...</td>\n",
       "      <td>在激活SOS功能时，需要按住特定按键约2秒才能手动激活该功能。当Lynk &amp; Co领克客户联...</td>\n",
       "      <td>[\\n## 手动SOS呼叫\\n\\n按住\\n\\n按键约2 秒，手动激活SOS功能。\\n\\nSO...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline</td>\n",
       "      <td>如何通过Lynk &amp; Co App操作前排座椅加热功能？请描述具体步骤。</td>\n",
       "      <td>要通过Lynk &amp; Co App操作前排座椅加热功能，可以按照以下步骤进行：\\n\\n1. 登...</td>\n",
       "      <td>[# 前排座椅加热\\n\\n## 使用Lynk &amp; Co App打开/关闭前排座椅加热\\n\\n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rag                                              query  \\\n",
       "0  baseline        根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。   \n",
       "1  baseline  在激活SOS功能时，需要按住哪个按键多长时间才能手动激活该功能？当Lynk & Co领克客户...   \n",
       "2  baseline               如何通过Lynk & Co App操作前排座椅加热功能？请描述具体步骤。   \n",
       "\n",
       "                                              answer  \\\n",
       "0  如果制动液液位低于储液罐的最低液位，应立即停止驾驶，并联系Lynk & Co领克中心进行检查...   \n",
       "1  在激活SOS功能时，需要按住特定按键约2秒才能手动激活该功能。当Lynk & Co领克客户联...   \n",
       "2  要通过Lynk & Co App操作前排座椅加热功能，可以按照以下步骤进行：\\n\\n1. 登...   \n",
       "\n",
       "                                            contexts  scores feedbacks  \n",
       "0  [# 检查制动液\\n\\n1 检查制动液液位。\\n\\n\\n请不定期地检查制动液液位，确保液位在...     0.0        NO  \n",
       "1  [\\n## 手动SOS呼叫\\n\\n按住\\n\\n按键约2 秒，手动激活SOS功能。\\n\\nSO...     1.0       YES  \n",
       "2  [# 前排座椅加热\\n\\n## 使用Lynk & Co App打开/关闭前排座椅加热\\n\\n...     1.0       YES  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_dfs[\"faithfulness\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用BatchEvalRunner进行批量评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:53<00:00,  2.83s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "runner=BatchEvalRunner(\n",
    "    evaluators={\n",
    "        \"answer_relevancy\":AnswerRelevancyEvaluator(),\n",
    "        \"context_relevancy\":ContextRelevancyEvaluator(),\n",
    "        \"correctness\":CorrectnessEvaluator(),\n",
    "        \"faithfulness\":FaithfulnessEvaluator()        \n",
    "    },\n",
    "    workers=8,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# 没有回答时，传入query_engine回答后评估\n",
    "# eval_results=await runner.aevaluate_queries(\n",
    "#     query_engine=query_engine,\n",
    "#     queries=[example.query for example in rag_dataset.examples],\n",
    "# )\n",
    "\n",
    "# 已有回答，直接传入回答评估\n",
    "eval_results=await runner.aevaluate_response_strs(\n",
    "    queries=[example.query for example in rag_dataset.examples],\n",
    "    contexts_list=[example.reference_contexts for example in rag_dataset.examples],\n",
    "    response_strs=[example.response for example in rag_predictions_dataset.predictions]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wushaogui\\AppData\\Local\\Temp\\ipykernel_59152\\2287145998.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  eval_results['answer_relevancy'][0].dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': '根据文档内容，如果制动液液位低于储液罐的最低液位，应该如何处理？请描述具体的应对措施。',\n",
       " 'contexts': None,\n",
       " 'response': '如果制动液液位低于储液罐的最低液位，应立即停止驾驶，并联系Lynk & Co领克中心进行检查。不要继续使用车辆，因为制动液液位下降明显可能意味着制动系统存在故障。',\n",
       " 'passing': None,\n",
       " 'feedback': \"1. **Does the provided response match the subject matter of the user's query?**\\n   - The response addresses the issue related to brake fluid level being below the minimum mark in a reservoir, which is directly relevant to the user's query.\\n   \\n2. **Does the provided response attempt to address the focus or perspective on the subject matter taken on by the user's query?**\\n   - The response focuses on the appropriate immediate action (stopping driving and contacting professional service) and explains the potential consequences of ignoring this issue, which aligns with addressing the concern about brake fluid levels.\\n\\nFeedback: The response is relevant to the query as it directly addresses the specific issue of low brake fluid level. It provides clear guidance on what actions should be taken when this occurs, considering both safety and maintenance aspects. \\n\\n[RESULT] 2\",\n",
       " 'score': 1.0,\n",
       " 'pairwise_source': None,\n",
       " 'invalid_result': False,\n",
       " 'invalid_reason': None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results['answer_relevancy'][0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_results(key, eval_results):\n",
    "    results = eval_results[key]\n",
    "    scores = 0\n",
    "    for result in results:\n",
    "        score = getattr(result,'score',0)\n",
    "        if score:\n",
    "            scores += score\n",
    "    score = scores / len(results)\n",
    "    print(f\"{key} Score: {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy Score: 0.95\n",
      "context_relevancy Score: 0.7525000000000001\n",
      "correctness Score: 3.5\n",
      "faithfulness Score: 0.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_eval_results('answer_relevancy',eval_results),\n",
    "get_eval_results('context_relevancy',eval_results),\n",
    "get_eval_results('correctness',eval_results),\n",
    "get_eval_results('faithfulness',eval_results),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
