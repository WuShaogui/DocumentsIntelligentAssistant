{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验结果汇总\n",
    "\n",
    "|检索器|topK|hit_rate|mrr|precision|recall|ap|ndcg|备注|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|VectorStoreIndex|2|0.779751|0.708703|0.400829|0.779751|0.708703|0.727308||\n",
    "|VectorStoreIndex|3|0.832445|0.725873|0.284784|0.832445|0.725873|0.753387||\n",
    "|VectorStoreIndex|5|0.885731|0.73954|0.184162|0.885731|0.73954|0.776535||\n",
    "|VectorStoreIndex|5|0.727649|0.628705|0.14998|0.727649|0.628705|0.653534|VectorStoreQueryMode.LINEAR_REGRESSION|\n",
    "|VectorStoreIndex|5|0.787448|0.684192|0.16269|0.787448|0.684192|0.709495|VectorStoreQueryMode.MMR|\n",
    "|SummaryIndex|5|0.971581|0.012225|0.001776|0.971581|0.012225|0.134562||\n",
    "|BM25Retriever|2|0.092954|0.086738|0.048253|0.092954|0.086738|0.088366|官方|\n",
    "|BM25Retriever|2|0.871522|0.806986|0.448194|0.871522|0.806986|0.823886|自定义|\n",
    "|BM25Retriever+VectorStoreIndex|2,2,2|0.872114|0.809355|0.44701|0.872114|0.809355|0.825789|自定义+QueryFusionRetriever|\n",
    "|BM25Retriever+VectorStoreIndex|3,5,2|0.870337|0.804914|0.435169|0.870337|0.804914|0.822046|自定义+QueryFusionRetriever|\n",
    "|BM25Retriever+VectorStoreIndex|3,5,3|0.907046|0.81715|0.302349|0.907046|0.81715|0.8404|自定义+QueryFusionRetriever|\n",
    "|BM25Retriever+VectorStoreIndex|3,5,3|0.910598|0.800276|0.303533|0.910598|0.800276|0.828861|FUSION_MODES.RECIPROCAL_RANK|\n",
    "|BM25Retriever+VectorStoreIndex|3,5,3|0.910006|0.799092|0.303335|0.910006|0.799092|0.82779|num_queries=2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "123"
    ]
   },
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T03:51:41.205497Z",
     "iopub.status.busy": "2024-12-24T03:51:41.204496Z",
     "iopub.status.idle": "2024-12-24T03:51:47.549809Z",
     "shell.execute_reply": "2024-12-24T03:51:47.548784Z",
     "shell.execute_reply.started": "2024-12-24T03:51:41.205497Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext,load_index_from_storage\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_qa_embedding_pairs,\n",
    "    EmbeddingQAFinetuneDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T03:52:03.497940Z",
     "iopub.status.busy": "2024-12-24T03:52:03.497940Z",
     "iopub.status.idle": "2024-12-24T03:52:07.317833Z",
     "shell.execute_reply": "2024-12-24T03:52:07.316833Z",
     "shell.execute_reply.started": "2024-12-24T03:52:03.497940Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置llm模型\n",
    "llm_name=\"qwen2.5:latest\"\n",
    "embedding_name=\"quentinz/bge-large-zh-v1.5:latest\"\n",
    "base_url='http://localhost:11434'\n",
    "\n",
    "Settings.llm = Ollama(\n",
    "    model=llm_name, \n",
    "    request_timeout=360.0,\n",
    "    base_url=base_url)\n",
    "\n",
    "# 设置embedding model \n",
    "Settings.embed_model = OllamaEmbedding(\n",
    "    model_name=embedding_name,\n",
    "    base_url=base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## Documents->nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T03:52:14.350116Z",
     "iopub.status.busy": "2024-12-24T03:52:14.350116Z",
     "iopub.status.idle": "2024-12-24T03:52:14.443219Z",
     "shell.execute_reply": "2024-12-24T03:52:14.441287Z",
     "shell.execute_reply.started": "2024-12-24T03:52:14.350116Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563 Node ID: 2d8f4884-c654-4554-9652-5b4d7d3d9cc7\n",
      "Text: ## 打开/关闭全景天窗   手动滑动打开（轻按按钮至第1个停止位置）。  自动滑动打开（按到底）。\n",
      "手动滑动关闭（轻按按钮至第1个停止位置）。  自动滑动关闭（按到底）。  如果全景天窗和遮阳帘处于完全关闭状态，轻按控制按钮，先打开遮\n",
      "阳帘，只有再次按下控制按钮后，才能打开全景天窗。  如果全景天窗和遮阳帘处于完全关闭状态，短时间内将控制按钮按到\n",
      "底两次，遮阳帘和全景天窗同时打开。\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from copy import deepcopy\n",
    "from llama_index.core.schema import Document,TextNode\n",
    "\n",
    "def split_markdown_by_headers(markdown_text):\n",
    "    # 正则表达式匹配Markdown的一级和二级标题\n",
    "    headers = re.compile(r'^(#+) (.*)$', re.MULTILINE)\n",
    "    \n",
    "    # 用于存储结果的列表\n",
    "    headers_content = {}\n",
    "    current_block = []\n",
    "    current_title=''\n",
    "    no_content_h1=''\n",
    "    \n",
    "    # 按行分割文档\n",
    "    lines = markdown_text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        # 检查当前行是否是标题\n",
    "        match = headers.match(line)\n",
    "        if match:\n",
    "            # 如果是标题，保存当前块（如果有的话）\n",
    "            if current_block:\n",
    "                if len(current_block)<=2:\n",
    "                    no_content_h1=current_block[0]\n",
    "                else:\n",
    "                    headers_content[current_title]=no_content_h1+'\\n'+''.join(current_block)\n",
    "                    current_block=[]\n",
    "            current_title=match.string\n",
    "            \n",
    "        # 如果不是标题，添加到当前块\n",
    "        current_block.append(line + '\\n')\n",
    "    \n",
    "    # 添加最后一个块\n",
    "    headers_content[current_title]=''.join(current_block)\n",
    "    \n",
    "    return headers_content\n",
    "\n",
    "def get_block_images(block):\n",
    "    images_path = re.findall(r'!\\[.*?\\]\\((.*?)\\)', block)\n",
    "    return images_path\n",
    "\n",
    "def process_content(headers_content):\n",
    "    nodes=[]\n",
    "    \n",
    "    for header in headers_content:\n",
    "        # 获取block的图片\n",
    "        block=headers_content[header]\n",
    "        images_path=get_block_images(block)\n",
    "\n",
    "        # 去掉block的图片文本\n",
    "        pattern = r\"!\\[.*\\)\"\n",
    "        block= re.sub(pattern, \"\\n\", block)\n",
    "\n",
    "        # 添加metadata：标题、内容等级、图片路径\n",
    "        metadata={\n",
    "            'title': block.split('\\n')[0].replace('#',''),\n",
    "            'content_level': header.count('#') ,\n",
    "            'images_path': images_path\n",
    "        }\n",
    "        \n",
    " \n",
    "        node=TextNode(\n",
    "            text=block,\n",
    "            metadata=deepcopy(metadata),\n",
    "        )\n",
    "        nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "def get_nodes_by_documents():\n",
    "    documents=[]\n",
    "    md_files=glob.glob('./preprocess/*.md')\n",
    "    for md_file in md_files:\n",
    "        with open(md_file,encoding='utf-8') as fr:\n",
    "            md_content='\\n'.join(fr.readlines())\n",
    "        documents.append(md_content)\n",
    "    \n",
    "    nodes=[]\n",
    "    for document in documents:\n",
    "        headers_content=split_markdown_by_headers(document)\n",
    "        document_nodes=process_content(headers_content)\n",
    "        nodes.extend(document_nodes)\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "\n",
    "nodes=get_nodes_by_documents()\n",
    "print(len(nodes),nodes[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nodes->indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T03:52:19.080547Z",
     "iopub.status.busy": "2024-12-24T03:52:19.080547Z",
     "iopub.status.idle": "2024-12-24T03:52:19.087547Z",
     "shell.execute_reply": "2024-12-24T03:52:19.087547Z",
     "shell.execute_reply.started": "2024-12-24T03:52:19.080547Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DocumentSummaryIndex', 'EmptyIndex', 'GPTDocumentSummaryIndex', 'GPTEmptyIndex', 'GPTKeywordTableIndex', 'GPTListIndex', 'GPTPandasIndex', 'GPTRAKEKeywordTableIndex', 'GPTSQLStructStoreIndex', 'GPTSimpleKeywordTableIndex', 'GPTTreeIndex', 'GPTVectorStoreIndex', 'KeywordTableIndex', 'KnowledgeGraphIndex', 'ListIndex', 'MultiModalVectorStoreIndex', 'PandasIndex', 'PropertyGraphIndex', 'RAKEKeywordTableIndex', 'SQLStructStoreIndex', 'SimpleKeywordTableIndex', 'SummaryIndex', 'TreeIndex', 'VectorStoreIndex']\n"
     ]
    }
   ],
   "source": [
    "# 查看所有的Index\n",
    "from llama_index.core import indices\n",
    "\n",
    "indexs=list(filter(lambda att:att.find('Index')>0,dir(indices)))\n",
    "print(indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def build_index(IndexType,nodes,persist_dir):\n",
    "    if os.path.exists(persist_dir):\n",
    "        storage_context=StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "        index=load_index_from_storage(storage_context=storage_context)\n",
    "    else:\n",
    "        index=IndexType(nodes=nodes,show_progress=True)\n",
    "        index.storage_context.persist(persist_dir=persist_dir)\n",
    "    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T03:52:31.561740Z",
     "iopub.status.busy": "2024-12-24T03:52:31.560772Z",
     "iopub.status.idle": "2024-12-24T03:52:32.935129Z",
     "shell.execute_reply": "2024-12-24T03:52:32.933156Z",
     "shell.execute_reply.started": "2024-12-24T03:52:31.561740Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def display_results(name,metrics, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "    # print(metric_df)\n",
    "\n",
    "    return metric_df\n",
    "\n",
    "def generate_qa_by_nodes(nodes,save_json_path='pg_eval_dataset.json'):\n",
    "    if os.path.exists(save_json_path):\n",
    "        qa_dataset=EmbeddingQAFinetuneDataset.from_json(path=save_json_path)\n",
    "    else:\n",
    "        # 生成测试数据\n",
    "        DEFAULT_QA_GENERATE_PROMPT_TMPL = \"\"\"\\\n",
    "        Context information is below.\n",
    "\n",
    "        ---------------------\n",
    "        {context_str}\n",
    "        ---------------------\n",
    "\n",
    "        Given the context information and not prior knowledge.\n",
    "        generate only questions based on the below query.\n",
    "\n",
    "        You are a Teacher/ Professor. Your task is to setup \\\n",
    "        {num_questions_per_chunk} questions for an upcoming \\\n",
    "        quiz/examination. The questions should be diverse in nature \\\n",
    "        across the document. Restrict the questions to the context information provided.\\\n",
    "        最终提问用中文生成\n",
    "        \"\"\"\n",
    "\n",
    "        qa_dataset=generate_qa_embedding_pairs(\n",
    "            nodes=nodes,\n",
    "            qa_generate_prompt_tmpl=DEFAULT_QA_GENERATE_PROMPT_TMPL,\n",
    "            num_questions_per_chunk=3)\n",
    "        qa_dataset.save_json(save_json_path)\n",
    "    \n",
    "    return qa_dataset\n",
    "\n",
    "async def MyRetrieverEvaluator(retriever,qa_dataset):\n",
    "\n",
    "    metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        metrics, retriever=retriever\n",
    "    )\n",
    "\n",
    "    # try it out on an entire dataset\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset,workers=8,show_progress=True)\n",
    "\n",
    "    return (eval_results,metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成测试数据\n",
    "qa_dataset=generate_qa_by_nodes(nodes,save_json_path='test_retriever_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1689/1689 [00:42<00:00, 39.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VectorStoreIndex</td>\n",
       "      <td>0.885731</td>\n",
       "      <td>0.739836</td>\n",
       "      <td>0.184162</td>\n",
       "      <td>0.885731</td>\n",
       "      <td>0.739836</td>\n",
       "      <td>0.776753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  VectorStoreIndex  0.885731  0.739836   0.184162  0.885731  0.739836   \n",
       "\n",
       "       ndcg  \n",
       "0  0.776753  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.indices import VectorStoreIndex\n",
    "\n",
    "index_type=VectorStoreIndex\n",
    "index_name=index_type.__name__\n",
    "index=build_index(index_type,nodes,os.path.join('./Storage',index_name))\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5 ,verbose=True)\n",
    "(eval_results,metrics)= await MyRetrieverEvaluator(retriever,qa_dataset)\n",
    "\n",
    "display_results(index_name,metrics,eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SummaryIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1689/1689 [00:21<00:00, 78.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SummaryIndex</td>\n",
       "      <td>0.971581</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.971581</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.134562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     retrievers  hit_rate       mrr  precision    recall        ap      ndcg\n",
       "0  SummaryIndex  0.971581  0.012225   0.001776  0.971581  0.012225  0.134562"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.indices import SummaryIndex\n",
    "\n",
    "index_type=SummaryIndex\n",
    "index_name=index_type.__name__\n",
    "index=build_index(index_type,nodes,os.path.join('./Storage',index_name))\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5 ,verbose=True)\n",
    "(eval_results,metrics)= await MyRetrieverEvaluator(retriever,qa_dataset)\n",
    "\n",
    "display_results(index_name,metrics,eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wushaogui\\miniconda3\\envs\\langchian\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from llama_index.core.storage.docstore.types import BaseDocumentStore\n",
    "from typing import Any, Callable, Dict, List, Optional, cast\n",
    "from llama_index.core.callbacks.base import CallbackManager\n",
    "from llama_index.core.constants import DEFAULT_SIMILARITY_TOP_K\n",
    "from llama_index.core.schema import BaseNode, IndexNode, NodeWithScore, QueryBundle\n",
    "from llama_index.core.vector_stores.utils import (\n",
    "    node_to_metadata_dict,\n",
    "    metadata_dict_to_node,\n",
    ")\n",
    "\n",
    "import itertools\n",
    "import jieba\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "class ChineseBM25Retriever(BM25Retriever):\n",
    "    \"\"\"A BM25 retriever that uses the BM25 algorithm to retrieve nodes.\n",
    "\n",
    "    Args:\n",
    "        nodes (List[BaseNode], optional):\n",
    "            The nodes to index. If not provided, an existing BM25 object must be passed.\n",
    "        similarity_top_k (int, optional):\n",
    "            The number of results to return. Defaults to DEFAULT_SIMILARITY_TOP_K.\n",
    "        callback_manager (CallbackManager, optional):\n",
    "            The callback manager to use. Defaults to None.\n",
    "        objects (List[IndexNode], optional):\n",
    "            The objects to retrieve. Defaults to None.\n",
    "        object_map (dict, optional):\n",
    "            A map of object IDs to nodes. Defaults to None.\n",
    "        verbose (bool, optional):\n",
    "            Whether to show progress. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def _chinese_tokenizer(self, texts: List[str]) -> tuple[str]:\n",
    "        # Use jieba to segment Chinese text\n",
    "        rslts = tuple(itertools.chain.from_iterable(jieba.cut(text) for text in texts))\n",
    "        return rslts\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            nodes: Optional[List[BaseNode]] = None,\n",
    "            similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n",
    "            callback_manager: Optional[CallbackManager] = None,\n",
    "            objects: Optional[List[IndexNode]] = None,\n",
    "            object_map: Optional[dict] = None,\n",
    "            verbose: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(\n",
    "            nodes=nodes,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            callback_manager=callback_manager,\n",
    "            objects=objects,\n",
    "            object_map=object_map,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        \n",
    "        # change the stop words for Chinese\n",
    "        with open(r'./stopwords-zh.txt', encoding='utf-8') as f: # here needs to add in the path of chinese stopwords\n",
    "            con = f.readlines()\n",
    "            stop_words = set()\n",
    "            for i in con:\n",
    "                i = i.rstrip('\\n')\n",
    "                stop_words.add(i)\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "        corpus_tokens = [\n",
    "            [word for word in jieba.cut_for_search(node.get_content()) if word not in stop_words and word.strip('\\n')]\n",
    "            for node in nodes\n",
    "        ]\n",
    "        corpus = [node_to_metadata_dict(node) for node in nodes]\n",
    "        self.bm25.corpus = corpus\n",
    "        self.bm25.index(corpus_tokens, show_progress=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_defaults(\n",
    "        cls,\n",
    "        index: Optional[VectorStoreIndex] = None,\n",
    "        nodes: Optional[List[BaseNode]] = None,\n",
    "        docstore: Optional[BaseDocumentStore] = None,\n",
    "        similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n",
    "        verbose: bool = False,\n",
    "    ) -> \"ChineseBM25Retriever\":\n",
    "          # ensure only one of index, nodes, or docstore is passed\n",
    "        if sum(bool(val) for val in [index, nodes, docstore]) != 1:\n",
    "            raise ValueError(\"Please pass exactly one of index, nodes, or docstore.\")\n",
    "\n",
    "        if index is not None:\n",
    "            docstore = index.docstore\n",
    "\n",
    "        if docstore is not None:\n",
    "            nodes = cast(List[BaseNode], list(docstore.docs.values()))\n",
    "        \n",
    "        assert (\n",
    "            nodes is not None\n",
    "        ), \"Please pass exactly one of index, nodes, or docstore.\"\n",
    "      \n",
    "        return cls(\n",
    "            nodes=nodes,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "      \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        query = query_bundle.query_str\n",
    "\n",
    "        tokenized_query = [[word for word in jieba.cut_for_search(query) if word not in self.stop_words]]\n",
    "\n",
    "        indexes, scores = self.bm25.retrieve(\n",
    "            tokenized_query, k=self.similarity_top_k, show_progress=self._verbose\n",
    "        )\n",
    "\n",
    "        # batched, but only one query\n",
    "        indexes = indexes[0]\n",
    "        scores = scores[0]\n",
    "\n",
    "        nodes: List[NodeWithScore] = []\n",
    "        for idx, score in zip(indexes, scores):\n",
    "            # idx can be an int or a dict of the node\n",
    "            if isinstance(idx, dict):\n",
    "                node = metadata_dict_to_node(idx)\n",
    "            else:\n",
    "                node_dict = self.corpus[int(idx)]\n",
    "                node = metadata_dict_to_node(node_dict)\n",
    "            nodes.append(NodeWithScore(node=node, score=float(score)))\n",
    "\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\WUSHAO~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.438 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 1689/1689 [00:00<00:00, 1773.98it/s]                   \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25Retriever</td>\n",
       "      <td>0.871522</td>\n",
       "      <td>0.806986</td>\n",
       "      <td>0.448194</td>\n",
       "      <td>0.871522</td>\n",
       "      <td>0.806986</td>\n",
       "      <td>0.823886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      retrievers  hit_rate       mrr  precision    recall        ap      ndcg\n",
       "0  BM25Retriever  0.871522  0.806986   0.448194  0.871522  0.806986  0.823886"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "index_type=VectorStoreIndex\n",
    "index_name=index_type.__name__\n",
    "index=build_index(index_type,nodes,os.path.join('./Storage',index_name))\n",
    "retriever = ChineseBM25Retriever.from_defaults(\n",
    "    index=index,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "\n",
    "(eval_results,metrics)= await MyRetrieverEvaluator(retriever,qa_dataset)\n",
    "\n",
    "index_name='BM25Retriever'\n",
    "display_results(index_name,metrics,eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QueryFusionRetriever=VectorStoreIndex+BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1689/1689 [00:47<00:00, 35.67it/s]                     \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25Retriever</td>\n",
       "      <td>0.910598</td>\n",
       "      <td>0.800276</td>\n",
       "      <td>0.303533</td>\n",
       "      <td>0.910598</td>\n",
       "      <td>0.800276</td>\n",
       "      <td>0.828861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      retrievers  hit_rate       mrr  precision    recall        ap      ndcg\n",
       "0  BM25Retriever  0.910598  0.800276   0.303533  0.910598  0.800276  0.828861"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.retrievers.fusion_retriever import FUSION_MODES\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [\n",
    "        index.as_retriever(similarity_top_k=5),\n",
    "        ChineseBM25Retriever.from_defaults(\n",
    "            index=index,\n",
    "            similarity_top_k=3\n",
    "),\n",
    "    ],\n",
    "    num_queries=1,\n",
    "    use_async=True,\n",
    "    similarity_top_k=3,\n",
    "    mode=FUSION_MODES.RECIPROCAL_RANK\n",
    ")\n",
    "\n",
    "(eval_results,metrics)= await MyRetrieverEvaluator(retriever,qa_dataset)\n",
    "\n",
    "index_name='BM25Retriever'\n",
    "display_results(index_name,metrics,eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
